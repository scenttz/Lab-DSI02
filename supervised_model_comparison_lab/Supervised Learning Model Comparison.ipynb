{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Supervised Learning Model Comparison\n",
    "\n",
    "_author The arbitrary and capricious heart of data science_\n",
    "\n",
    "---\n",
    "\n",
    "### Let us begin...\n",
    "\n",
    "Recall the \"data science process.\"\n",
    "   1. Define the problem.\n",
    "   2. Gather the data.\n",
    "   3. Explore the data.\n",
    "   4. Model the data.\n",
    "   5. Evaluate the model.\n",
    "   6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you.\n",
    "Most of the questions requiring a written response can be written in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Imports statistics\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor,\\\n",
    "RandomForestRegressor, AdaBoostClassifier, BaggingClassifier,\\\n",
    "RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,\\\n",
    "accuracy_score, plot_roc_curve, roc_auc_score, recall_score, \\\n",
    "precision_score, f1_score, classification_report, mean_squared_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dataset\n",
    "df = pd.read_csv(\"401ksubs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>98.880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9777.2540</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511.3930</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   e401k     inc  marr  male  age  fsize   nettfa  p401k  pira      incsq  \\\n",
       "0      0  13.170     0     0   40      1    4.575      0     1   173.4489   \n",
       "1      1  61.230     0     1   35      1  154.000      1     0  3749.1130   \n",
       "2      0  12.858     1     0   44      2    0.000      0     0   165.3282   \n",
       "3      0  98.880     1     1   44      2   21.800      0     0  9777.2540   \n",
       "4      0  22.614     0     0   53      1   18.450      0     0   511.3930   \n",
       "\n",
       "   agesq  \n",
       "0   1600  \n",
       "1   1225  \n",
       "2   1936  \n",
       "3   1936  \n",
       "4   2809  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View top 5 rows \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9275, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check row and columns\n",
    "# Large sample size\n",
    "# I'm comfortable divide training and testing my model 80% and 20% \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Other variables that would be helpful\n",
    "- Education\tand self employed \n",
    "- Personal investment such as stocks, bonds, property and others. It's show someone have knowledge for investment. \n",
    "- Owning property such as own house/condo, car etc. It's good indicator of someone have strong financial health and able to keep money."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "race : nationality\n",
    "\n",
    "This would be an unethical because if a certain nationality(race) makes less income just because the data was gathered, the model will have an inherent bias towards that nationality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "I wouldn't use `incsq` feature to predict income(`inc`) because `incsq` is similarly double values of `inc`. If we use both features in the model, the model can directly predict so what is wrong. we want to predict, not tell the model the answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs might have done this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>agesq</th>\n",
       "      <th>inc</th>\n",
       "      <th>incsq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>1600</td>\n",
       "      <td>13.170</td>\n",
       "      <td>173.4489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>1225</td>\n",
       "      <td>61.230</td>\n",
       "      <td>3749.1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>1936</td>\n",
       "      <td>12.858</td>\n",
       "      <td>165.3282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>1936</td>\n",
       "      <td>98.880</td>\n",
       "      <td>9777.2540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>2809</td>\n",
       "      <td>22.614</td>\n",
       "      <td>511.3930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  agesq     inc      incsq\n",
       "0   40   1600  13.170   173.4489\n",
       "1   35   1225  61.230  3749.1130\n",
       "2   44   1936  12.858   165.3282\n",
       "3   44   1936  98.880  9777.2540\n",
       "4   53   2809  22.614   511.3930"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"age\", \"agesq\",\"inc\",\"incsq\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "The features `incsq` and `agesq`. They may have done this to add emphasis to these features in their modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one variable description appears to be an error. What is this error, and what do you think the correct value would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `inc` is defined as `inc^2` in data dictionary but should refer to one's income\n",
    "- `age` is defined as `age^2` in data dictionary but should refer to one's age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all modeling tactics we've learned that could be used to solve a regression problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific regression problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "List all modeling tactics  \n",
    " - a multiple linear regression model(Yes, it can predict of features)  \n",
    " - a k-nearest neighbors model (No, it can't predict of features) \n",
    " - a decision tree (Yes, it can predict of features)  \n",
    " - a set of bagged decision trees (Yes, it can predict of features)(similar to decision tree model)  \n",
    " - a random forest(Yes, it can predict of features)(similar to decision tree model)    \n",
    " - an Adaboost model(Yes, it can predict of features)(similar to decision tree model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e401k     0\n",
       "inc       0\n",
       "marr      0\n",
       "male      0\n",
       "age       0\n",
       "fsize     0\n",
       "nettfa    0\n",
       "p401k     0\n",
       "pira      0\n",
       "incsq     0\n",
       "agesq     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing value\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e401k       int64\n",
       "inc       float64\n",
       "marr        int64\n",
       "male        int64\n",
       "age         int64\n",
       "fsize       int64\n",
       "nettfa    float64\n",
       "p401k       int64\n",
       "pira        int64\n",
       "incsq     float64\n",
       "agesq       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check types \n",
    "# Not have object type\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression: What features best predict one's income?\n",
    "# not use features same meaning in income\n",
    "# not use features tell participate in eligible 401k/IRA\n",
    "# Define X & y\n",
    "X = df.drop(columns=[\"inc\",\"incsq\", \"e401k\", \"p401k\",\"pira\"])\n",
    "y = df[\"inc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   marr  male  age  fsize   nettfa  agesq\n",
       "0     0     0   40      1    4.575   1600\n",
       "1     0     1   35      1  154.000   1225\n",
       "2     1     0   44      2    0.000   1936\n",
       "3     1     1   44      2   21.800   1936\n",
       "4     0     0   53      1   18.450   2809"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training & testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size =0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Knn model and use hyperpatameter of GridSearch\n",
    "# Must have scale our features \n",
    "# I decide use it all my model\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostRegressor</label><div class=\"sk-toggleable__content\"><pre>AdaBoostRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostRegressor()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate & fit our models\n",
    "\n",
    "# Linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_sc, y_train)\n",
    "\n",
    "# k-nearest neighbors model\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_sc, y_train)\n",
    "\n",
    "# Decision tree\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(X_train_sc, y_train)\n",
    "\n",
    "# Set of bagged decision trees\n",
    "bag = BaggingRegressor()\n",
    "bag.fit(X_train_sc, y_train)\n",
    "\n",
    "# Random forest\n",
    "rdf = RandomForestRegressor()\n",
    "rdf.fit(X_train_sc, y_train)\n",
    "\n",
    "# Adaboost model\n",
    "ada = AdaBoostRegressor()\n",
    "ada.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>R2_training</th>\n",
       "      <th>R2_testing</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.275</td>\n",
       "      <td>10.297189</td>\n",
       "      <td>1.221685</td>\n",
       "      <td>31.549951</td>\n",
       "      <td>-3.277678</td>\n",
       "      <td>8.153784</td>\n",
       "      <td>-31.2624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsRegressor</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>0.991</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.104979</td>\n",
       "      <td>0.017113</td>\n",
       "      <td>0.103982</td>\n",
       "      <td>0.065292</td>\n",
       "      <td>0.609194</td>\n",
       "      <td>0.09944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BaggingRegressor</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.104379</td>\n",
       "      <td>0.021274</td>\n",
       "      <td>0.099212</td>\n",
       "      <td>0.068824</td>\n",
       "      <td>0.606589</td>\n",
       "      <td>0.099723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoostRegressor</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.157077</td>\n",
       "      <td>0.023726</td>\n",
       "      <td>0.052208</td>\n",
       "      <td>0.009793</td>\n",
       "      <td>0.712831</td>\n",
       "      <td>0.044364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model  R2_training  R2_testing       marr      male  \\\n",
       "0       LinearRegression        0.293       0.275  10.297189  1.221685   \n",
       "1    KNeighborsRegressor        0.526       0.324          -         -   \n",
       "2  DecisionTreeRegressor        0.991      -0.236   0.104979  0.017113   \n",
       "3       BaggingRegressor        0.863       0.286          -         -   \n",
       "4  RandomForestRegressor        0.896       0.316   0.104379  0.021274   \n",
       "5      AdaBoostRegressor        0.261       0.224   0.157077  0.023726   \n",
       "\n",
       "         age     fsize    nettfa     agesq  \n",
       "0  31.549951 -3.277678  8.153784  -31.2624  \n",
       "1          -         -         -         -  \n",
       "2   0.103982  0.065292  0.609194   0.09944  \n",
       "3          -         -         -         -  \n",
       "4   0.099212  0.068824  0.606589  0.099723  \n",
       "5   0.052208  0.009793  0.712831  0.044364  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of our models\n",
    "model_list_reg = [lr, knn, dtr, bag, rdf, ada]\n",
    "\n",
    "# Dict columns in DataFrame\n",
    "model_table_dict = {'model':[], 'R2_training': [], 'R2_testing':[],\n",
    "                   'marr':[], 'male':[], 'age':[], 'fsize':[], 'nettfa':[], \n",
    "                    'agesq':[]}\n",
    "\n",
    "# Make Dataframe of our models\n",
    "# Show R^2 of model training and testing\n",
    "# Show features importance of each models\n",
    "for i in model_list_reg:\n",
    "    model_table_dict['model'].append(str(i).split(\"()\")[0])\n",
    "    model_table_dict['R2_training'].append(round(i.score(X_train_sc, y_train),3))\n",
    "    model_table_dict['R2_testing'].append(round(i.score(X_test_sc, y_test),3))\n",
    "    \n",
    "    # lr use coef_ function\n",
    "    if i == lr:\n",
    "        model_table_dict['marr'].append(i.coef_[0])\n",
    "        model_table_dict['male'].append(i.coef_[1])\n",
    "        model_table_dict['age'].append(i.coef_[2])\n",
    "        model_table_dict['fsize'].append(i.coef_[3])\n",
    "        model_table_dict['nettfa'].append(i.coef_[4])\n",
    "        model_table_dict['agesq'].append(i.coef_[5])\n",
    "        \n",
    "    # Group dtr/rdf/ada use feature_importances_ function\n",
    "    elif i == dtr or i == rdf or i == ada: \n",
    "        model_table_dict['marr'].append(i.feature_importances_[0])\n",
    "        model_table_dict['male'].append(i.feature_importances_[1])\n",
    "        model_table_dict['age'].append(i.feature_importances_[2])\n",
    "        model_table_dict['fsize'].append(i.feature_importances_[3])\n",
    "        model_table_dict['nettfa'].append(i.feature_importances_[4])\n",
    "        model_table_dict['agesq'].append(i.feature_importances_[5])\n",
    "    \n",
    "    # Knn and Bag can't predict important feature\n",
    "    else: \n",
    "        model_table_dict['marr'].append(\"-\")\n",
    "        model_table_dict['male'].append(\"-\")\n",
    "        model_table_dict['age'].append(\"-\")\n",
    "        model_table_dict['fsize'].append(\"-\")\n",
    "        model_table_dict['nettfa'].append(\"-\")\n",
    "        model_table_dict['agesq'].append(\"-\")\n",
    "        \n",
    "model_df = pd.DataFrame(model_table_dict)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model          LinearRegression\n",
       "R2_training               0.293\n",
       "R2_testing                0.275\n",
       "marr                  10.297189\n",
       "male                   1.221685\n",
       "age                   31.549951\n",
       "fsize                 -3.277678\n",
       "nettfa                 8.153784\n",
       "agesq                  -31.2624\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LinearRegression model\n",
    "# Not overfitting \n",
    "# R^2 for training/testing is very similar\n",
    "# Most important feature : (age)\n",
    "model_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model          KNeighborsRegressor\n",
       "R2_training                  0.526\n",
       "R2_testing                   0.324\n",
       "marr                             -\n",
       "male                             -\n",
       "age                              -\n",
       "fsize                            -\n",
       "nettfa                           -\n",
       "agesq                            -\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNeighborsRegressormodel\n",
    "# slightly overfitting \n",
    "# R^2 of training is more than testing \n",
    "model_df.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model          DecisionTreeRegressor\n",
       "R2_training                    0.991\n",
       "R2_testing                    -0.236\n",
       "marr                        0.104979\n",
       "male                        0.017113\n",
       "age                         0.103982\n",
       "fsize                       0.065292\n",
       "nettfa                      0.609194\n",
       "agesq                        0.09944\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DecisionTreeRegressor model\n",
    "# Overfitting !! \n",
    "# R^2 of training is very more than testing \n",
    "# Most important feature : (nettfa)\n",
    "model_df.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model          BaggingRegressor\n",
       "R2_training               0.863\n",
       "R2_testing                0.286\n",
       "marr                          -\n",
       "male                          -\n",
       "age                           -\n",
       "fsize                         -\n",
       "nettfa                        -\n",
       "agesq                         -\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BaggingRegressor model\n",
    "# Overfitting\n",
    "# R^2 of training is more than testing \n",
    "model_df.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model          RandomForestRegressor\n",
       "R2_training                    0.896\n",
       "R2_testing                     0.316\n",
       "marr                        0.104379\n",
       "male                        0.021274\n",
       "age                         0.099212\n",
       "fsize                       0.068824\n",
       "nettfa                      0.606589\n",
       "agesq                       0.099723\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestRegressor model\n",
    "# Overfitting\n",
    "# R^2 of training is more than testing \n",
    "# Most important feature : (nettfa)\n",
    "model_df.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model          AdaBoostRegressor\n",
       "R2_training                0.261\n",
       "R2_testing                 0.224\n",
       "marr                    0.157077\n",
       "male                    0.023726\n",
       "age                     0.052208\n",
       "fsize                   0.009793\n",
       "nettfa                  0.712831\n",
       "agesq                   0.044364\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AdaBoostRegressor model\n",
    "# Not overfitting \n",
    "# R^2 for training/testing is very similar\n",
    "# Most important feature : (nettfa)\n",
    "model_df.iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "- The bootstrap is a statistical technique that is used to quantify the uncertainty of a model\n",
    "- Bootstrap samples are simply random samples with replacement that the same size as our original sample but are not replicas or exactly the same(averaged out the predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "- Decision trees to produce better predictive performance than utilizing a single decision tree.\n",
    "\n",
    "- Set of bagged decision trees idea is to create several subsets(B model) of data from training sample chosen`features` randomly with replacement. Now, each collection of subset data is used to train their decision trees. As a result, we end up with an ensemble of different models. Average of all the `predictions from different trees are used which is more robust than a single decision tree.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "- Random forest is based on applying a set of bagged decision trees with one important extension: In addition to sampling the `rows`, the algorithm also samples the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "- Performance of random forest is usually pretty strong for the classifier and the regressor more than set of bagged decision trees because reducing bias while not increasing variance too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>R2_training</th>\n",
       "      <th>R2_testing</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>agesq</th>\n",
       "      <th>RMSE_training</th>\n",
       "      <th>RMSE_testing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.275</td>\n",
       "      <td>10.297189</td>\n",
       "      <td>1.221685</td>\n",
       "      <td>31.549951</td>\n",
       "      <td>-3.277678</td>\n",
       "      <td>8.153784</td>\n",
       "      <td>-31.2624</td>\n",
       "      <td>20.16</td>\n",
       "      <td>20.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsRegressor</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>16.50</td>\n",
       "      <td>20.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>0.991</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.104979</td>\n",
       "      <td>0.017113</td>\n",
       "      <td>0.103982</td>\n",
       "      <td>0.065292</td>\n",
       "      <td>0.609194</td>\n",
       "      <td>0.09944</td>\n",
       "      <td>2.26</td>\n",
       "      <td>27.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BaggingRegressor</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>8.88</td>\n",
       "      <td>20.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.104379</td>\n",
       "      <td>0.021274</td>\n",
       "      <td>0.099212</td>\n",
       "      <td>0.068824</td>\n",
       "      <td>0.606589</td>\n",
       "      <td>0.099723</td>\n",
       "      <td>7.73</td>\n",
       "      <td>20.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoostRegressor</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.157077</td>\n",
       "      <td>0.023726</td>\n",
       "      <td>0.052208</td>\n",
       "      <td>0.009793</td>\n",
       "      <td>0.712831</td>\n",
       "      <td>0.044364</td>\n",
       "      <td>20.61</td>\n",
       "      <td>21.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model  R2_training  R2_testing       marr      male  \\\n",
       "0       LinearRegression        0.293       0.275  10.297189  1.221685   \n",
       "1    KNeighborsRegressor        0.526       0.324          -         -   \n",
       "2  DecisionTreeRegressor        0.991      -0.236   0.104979  0.017113   \n",
       "3       BaggingRegressor        0.863       0.286          -         -   \n",
       "4  RandomForestRegressor        0.896       0.316   0.104379  0.021274   \n",
       "5      AdaBoostRegressor        0.261       0.224   0.157077  0.023726   \n",
       "\n",
       "         age     fsize    nettfa     agesq  RMSE_training  RMSE_testing  \n",
       "0  31.549951 -3.277678  8.153784  -31.2624          20.16         20.90  \n",
       "1          -         -         -         -          16.50         20.18  \n",
       "2   0.103982  0.065292  0.609194   0.09944           2.26         27.28  \n",
       "3          -         -         -         -           8.88         20.73  \n",
       "4   0.099212  0.068824  0.606589  0.099723           7.73         20.30  \n",
       "5   0.052208  0.009793  0.712831  0.044364          20.61         21.62  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add RSME on dict columns in DataFrame\n",
    "model_table_dict.update({'RMSE_training': [], 'RMSE_testing':[]})\n",
    "\n",
    "# Make Dataframe of our models\n",
    "# Add RSME values\n",
    "for i in model_list_reg:\n",
    "    y_pred_train = i.predict(X_train_sc)\n",
    "    y_pred_test = i.predict(X_test_sc)\n",
    "    model_table_dict['RMSE_training'].append(round(mean_squared_error(y_train, y_pred_train, squared=False),2))\n",
    "    model_table_dict['RMSE_testing'].append(round(mean_squared_error(y_test, y_pred_test, squared=False),2))\n",
    "\n",
    "model_df = pd.DataFrame(model_table_dict)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If RSME is low, It's good to predict income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model            LinearRegression\n",
       "RMSE_training               20.16\n",
       "RMSE_testing                 20.9\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LinearRegression model\n",
    "# Not overfitting \n",
    "# RMSE for training/testing is very similar\n",
    "model_df.loc[0, ['model',\"RMSE_training\",\"RMSE_testing\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model            KNeighborsRegressor\n",
       "RMSE_training                   16.5\n",
       "RMSE_testing                   20.18\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNeighborsRegressormodel\n",
    "# slightly overfitting \n",
    "# RSME of training is lower than testing    \n",
    "model_df.loc[1, ['model',\"RMSE_training\",\"RMSE_testing\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model            DecisionTreeRegressor\n",
       "RMSE_training                     2.26\n",
       "RMSE_testing                     27.28\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DecisionTreeRegressor model\n",
    "# Overfitting !!\n",
    "# RSME of training is very lower than testing\n",
    "model_df.loc[2, ['model',\"RMSE_training\",\"RMSE_testing\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model            BaggingRegressor\n",
       "RMSE_training                8.88\n",
       "RMSE_testing                20.73\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BaggingRegressor model\n",
    "# Overfitting !!\n",
    "# RSME of training is very lower than testing\n",
    "model_df.loc[3, ['model',\"RMSE_training\",\"RMSE_testing\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model            RandomForestRegressor\n",
       "RMSE_training                     7.73\n",
       "RMSE_testing                      20.3\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestRegressor model\n",
    "# Overfitting !!\n",
    "# RSME of training is very lower than testing\n",
    "model_df.loc[4, ['model',\"RMSE_training\",\"RMSE_testing\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model            AdaBoostRegressor\n",
       "RMSE_training                20.61\n",
       "RMSE_testing                 21.62\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AdaBoostRegressor model\n",
    "# Not overfitting \n",
    "# RMSE for training/testing is very similar\n",
    "model_df.loc[5, ['model',\"RMSE_training\",\"RMSE_testing\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model            RandomForestRegressor\n",
       "R2_training                      0.896\n",
       "R2_testing                       0.316\n",
       "RMSE_training                     7.73\n",
       "RMSE_testing                      20.3\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick RandomForestRegressor model to final model\n",
    "# It had one of the lowest RMSE scores and highest R^2. (Good prediction)\n",
    "# Even if it's overfiiting (High Variance)\n",
    "model_df.loc[4, ['model',\"R2_training\",\"R2_testing\",\"RMSE_training\",\"RMSE_testing\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "- First I would check for outliers in the dataset. \n",
    "- Secondly I would perform a Gridsearch to find the best parameters for this \n",
    "model.\n",
    "- Thirdly I would like to use feature engineer considering transforming income(using Log scale).  \n",
    "- Lastly I would collect more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "If we include whether or not someone already possesses a 401k, then every person with a 401k must by definition be eligible for a 401k. However, this probably doesn't contribute to our understanding of what makes someone eligible for a 401k and will only confound what we're actually interested in studyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     99.967655\n",
       "False     0.032345\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Someone is already possesses a 401k must be eligible for a 401k  \n",
    "# That is show 99.96 % same row in dataset\n",
    "# This probably doesn't contribute to our model that is more good predict\n",
    "df[[\"e401k\",\"p401k\"]].duplicated().value_counts(normalize=True).mul(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific classification problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "- a logistic regression model (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- a $k$-nearest neighbors model (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- a decision tree (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- a set of bagged decision trees (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- a random forest (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- a set of extremely randomized trees (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- an Adaboost model (Yes, we can predict whether or not one is eligible for a 401(k).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define X & y\n",
    "X = df.drop(columns=[\"e401k\", \"p401k\"])\n",
    "y = df[\"e401k\"]\n",
    "\n",
    "# Split into training & testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# In Knn model and use hyperpatameter of GridSearch\n",
    "# Must have scale our features \n",
    "# I decide use it all my model\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7420, 9), (1855, 9), (7420,), (1855,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    60.73\n",
       "1    39.27\n",
       "Name: e401k, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a baseline score \n",
    "# baseline score is ~61%\n",
    "y_train.value_counts(normalize=True).mul(100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = logreg.predict(X_test_sc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>true_negatives</th>\n",
       "      <th>false_positives</th>\n",
       "      <th>false_negatives</th>\n",
       "      <th>true_positives</th>\n",
       "      <th>F1_training</th>\n",
       "      <th>F1_testing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.663612</td>\n",
       "      <td>946</td>\n",
       "      <td>186</td>\n",
       "      <td>438</td>\n",
       "      <td>285</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.639353</td>\n",
       "      <td>856</td>\n",
       "      <td>276</td>\n",
       "      <td>393</td>\n",
       "      <td>330</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.582210</td>\n",
       "      <td>739</td>\n",
       "      <td>393</td>\n",
       "      <td>382</td>\n",
       "      <td>341</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>0.649057</td>\n",
       "      <td>877</td>\n",
       "      <td>255</td>\n",
       "      <td>396</td>\n",
       "      <td>327</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.659838</td>\n",
       "      <td>878</td>\n",
       "      <td>254</td>\n",
       "      <td>377</td>\n",
       "      <td>346</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.691105</td>\n",
       "      <td>904</td>\n",
       "      <td>228</td>\n",
       "      <td>345</td>\n",
       "      <td>378</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model  accuracy  true_negatives  false_positives  \\\n",
       "0      LogisticRegression  0.663612             946              186   \n",
       "1    KNeighborsClassifier  0.639353             856              276   \n",
       "2  DecisionTreeClassifier  0.582210             739              393   \n",
       "3       BaggingClassifier  0.649057             877              255   \n",
       "4  RandomForestClassifier  0.659838             878              254   \n",
       "5      AdaBoostClassifier  0.691105             904              228   \n",
       "\n",
       "   false_negatives  true_positives  F1_training  F1_testing  \n",
       "0              438             285         0.47        0.48  \n",
       "1              393             330         0.65        0.50  \n",
       "2              382             341         1.00        0.47  \n",
       "3              396             327         0.97        0.50  \n",
       "4              377             346         1.00        0.52  \n",
       "5              345             378         0.56        0.57  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of our models\n",
    "model_list_class = [LogisticRegression(),\n",
    "                    KNeighborsClassifier(),\n",
    "                    DecisionTreeClassifier(),\n",
    "                    BaggingClassifier(),\n",
    "                    RandomForestClassifier(),\n",
    "                    AdaBoostClassifier()]\n",
    "\n",
    "# Dict columns in DataFrame\n",
    "model_table_dict = {'model':[], 'accuracy': [], 'true_negatives':[], \n",
    "                  'false_positives':[],'false_negatives':[],'true_positives':[], \n",
    "                  'F1_training':[], 'F1_testing':[]}\n",
    "\n",
    "# Make Dataframe of our models\n",
    "# Show confusion_matrix of each model training and testing\n",
    "# Show f1-score of each models\n",
    "for i in model_list_class:\n",
    "    # fit\n",
    "    i.fit(X_train_sc, y_train)\n",
    "    \n",
    "    # predict\n",
    "    y_train_pred = i.predict(X_train_sc)\n",
    "    y_test_pred = i.predict(X_test_sc)\n",
    "    \n",
    "    # add values in dict\n",
    "    model_table_dict['model'].append(str(i).split(\"()\")[0])\n",
    "    model_table_dict['accuracy'].append(accuracy_score(y_test, y_test_pred))\n",
    "    model_table_dict['true_negatives'].append(confusion_matrix(y_test, y_test_pred).ravel()[0])\n",
    "    model_table_dict['false_positives'].append(confusion_matrix(y_test, y_test_pred).ravel()[1])\n",
    "    model_table_dict['false_negatives'].append(confusion_matrix(y_test, y_test_pred).ravel()[2])\n",
    "    model_table_dict['true_positives'].append(confusion_matrix(y_test, y_test_pred).ravel()[3])\n",
    "    model_table_dict['F1_training'].append(round(f1_score(y_train, y_train_pred),2))\n",
    "    model_table_dict['F1_testing'].append(round(f1_score(y_test, y_test_pred),2))\n",
    "\n",
    "# make Dataframe\n",
    "model_df = pd.DataFrame(model_table_dict)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "- False positives are people we incorrectly predict to be eligible for a 401k.\n",
    "- False negatives are people we incorrectly predict to be ineligible for a 401k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "- We would to predict to be eligible for a 401k.\n",
    "- We would rather minimize our false positive because we would want to minimize as much risk as we can.\n",
    "- People who aren't eligible for 401k's being able to open them up would be bad for business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "- If I want to minimize false positives, then I want to optimize specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "- It balances our false positives and false negatives. \n",
    "- As either false positives or false negatives increase, the denominator increases \n",
    "- while the numerator stays fixed, meaning our 𝐹1-score decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data. and  25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model              LogisticRegression\n",
       "accuracy                     0.663612\n",
       "true_negatives                    946\n",
       "false_positives                   186\n",
       "false_negatives                   438\n",
       "true_positives                    285\n",
       "F1_training                      0.47\n",
       "F1_testing                       0.48\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression model\n",
    "# Accuracy score is ~66% more than baseline score\n",
    "# Not overfitting\n",
    "# F1-score for training/testing is very similar\n",
    "model_df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model              KNeighborsClassifier\n",
       "accuracy                       0.639353\n",
       "true_negatives                      856\n",
       "false_positives                     276\n",
       "false_negatives                     393\n",
       "true_positives                      330\n",
       "F1_training                        0.65\n",
       "F1_testing                          0.5\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNeighborsClassifier model\n",
    "# Accuracy score is ~64% more than baseline score\n",
    "# Slightly overfitting\n",
    "# F1-score of training is higher than testing \n",
    "model_df.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model              DecisionTreeClassifier\n",
       "accuracy                          0.58221\n",
       "true_negatives                        739\n",
       "false_positives                       393\n",
       "false_negatives                       382\n",
       "true_positives                        341\n",
       "F1_training                           1.0\n",
       "F1_testing                           0.47\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DecisionTreeClassifier\n",
    "# Accuracy score is ~58% lower than baseline score\n",
    "# Very overfitting\n",
    "# F1-score of training is very higher than testing \n",
    "model_df.loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model              BaggingClassifier\n",
       "accuracy                    0.649057\n",
       "true_negatives                   877\n",
       "false_positives                  255\n",
       "false_negatives                  396\n",
       "true_positives                   327\n",
       "F1_training                     0.97\n",
       "F1_testing                       0.5\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BaggingClassifier\n",
    "# Accuracy score is ~64% more than baseline score\n",
    "# Very overfitting\n",
    "# F1-score of training is very higher than testing \n",
    "model_df.loc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model              RandomForestClassifier\n",
       "accuracy                         0.659838\n",
       "true_negatives                        878\n",
       "false_positives                       254\n",
       "false_negatives                       377\n",
       "true_positives                        346\n",
       "F1_training                           1.0\n",
       "F1_testing                           0.52\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "# Accuracy score is ~66% more than baseline score\n",
    "# Very overfitting\n",
    "# F1-score of training is very higher than testing \n",
    "model_df.loc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model              AdaBoostClassifier\n",
       "accuracy                     0.691105\n",
       "true_negatives                    904\n",
       "false_positives                   228\n",
       "false_negatives                   345\n",
       "true_positives                    378\n",
       "F1_training                      0.56\n",
       "F1_testing                       0.57\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AdaBoostClassifier\n",
    "# Accuracy score is ~69% more than baseline score and the highest of models \n",
    "# Not overfitting\n",
    "# F1-score for training/testing is very similar\n",
    "model_df.loc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model              AdaBoostClassifier\n",
       "accuracy                     0.691105\n",
       "true_negatives                    904\n",
       "false_positives                   228\n",
       "false_negatives                   345\n",
       "true_positives                    378\n",
       "F1_training                      0.56\n",
       "F1_testing                       0.57\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick AdaBoostClassifier model to final model\n",
    "# Accuracy score is the highest models (Good prediction)\n",
    "# F1-score is acceptable (Good prediction)\n",
    "# Not overfitting\n",
    "model_df.loc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First I would check for outliers in the dataset.\n",
    "- Secondly I would perform a Gridsearch to find the best hyperparamters for this model.\n",
    "- Lastly I would collect more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Answer the problem.\n",
    "\n",
    "##### BONUS: Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Regression: What features best predict one's income?\n",
    "# The feature as best predicts one's income is Net Total Financial Assets.\n",
    "# That is reasonable because people have high net total financial assets \n",
    "# So they have knowledge of investment and saving money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification: Predict whether or not one is eligible for a 401k.\n",
    "# We can predict eligibility for a 401k approximately 69 percent.\n",
    "# However we can improve our model by used Gridsearch to find the best hyperparamters "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
